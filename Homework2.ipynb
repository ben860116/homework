{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conceptual\n",
    "\n",
    "   1. What is the difference between train and test set?\n",
    "   Training set is to build the model, and test set is to validate the data build.\n",
    "   \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   2.Why is it important to have a seperate data set for training?\n",
    "   \n",
    "      After the seperation we can better understand the model, and minimizing the overfit.\n",
    "      It also serves as check on the overfit, give estimate of performance on an independant dataset.\n",
    "   \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. What are some advantages and disadvantages of the KNN algorithm?\n",
    "    \n",
    "    advantages:\n",
    "                   1. Training is fast\n",
    "                   2. Learn complex target functions\n",
    "                   3. Dont lose information.\n",
    "    Disadvantages:\n",
    "                   1. Slow at query time\n",
    "                   2. Storage issues\n",
    "                   3. easily fooled by irrelevant features and high dimensions.\n",
    "                   \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Explain in your own words the bias variance tradeoff.\n",
    "    \n",
    "        Variance: it can only simulate the things seen before, overfit.\n",
    "        Bias: Ignores the dataset features.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.  Do you think KNN suffers from high bias or high variance?\n",
    "    \n",
    "    \n",
    "        Depends on the K, when K is small it is bias, when K is big, it is variance.\n",
    "     \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.List some possible solutions when suffering from high variance.\n",
    "    \n",
    "    Get more data or decrease more features.\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.List some possible solutions when suffereing from high bias.\n",
    "   \n",
    "     1. find more features.\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.Would you choose K when doing K Nearest Neighbors?\n",
    "\n",
    " K fold validataion do that.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2 :\n",
    "\n",
    "1.Perform regression on the prostate data set. Your goal is to minimize the mean squared error (MSE). Split this into a several parts and at a minimum utilize\n",
    "\n",
    "    Linear Regression\n",
    "    KNN\n",
    "    Lasso\n",
    "    Ridge\n",
    "    Use cross validation to tune the best model you can\n",
    "    \n",
    "    \n",
    "    \n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pandas import *\n",
    "\n",
    "data = read_csv('https://raw.githubusercontent.com/galvin-mj/DAT_ATL_15/master/Datasets/ProstateCancer.csv')\n",
    "del data['Unnamed: 0']\n",
    "train = data[data['train']=='T']\n",
    "test = data[data['train']=='F']\n",
    "X_train = train[['lcavol','lweight','age','lbph','svi','gleason','pgg45']]\n",
    "y_train = train[['lpsa']]\n",
    "X_test = test[['lcavol','lweight','age','lbph','svi','gleason','pgg45']]\n",
    "y_test = test[['lpsa']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coefficient [[ 0.48908263  0.61223896 -0.01713236  0.15436282  0.55160293 -0.00848756\n",
      "   0.00550287]]\n",
      "intercept [ 0.47720966]\n",
      "Mean squared error for liner regression: 0.461141049694\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "lr_rgr = linear_model.LinearRegression()\n",
    "lr_rgr.fit(X_train,y_train)\n",
    "print(\"coefficient {}\".format(lr_rgr.coef_))\n",
    "print(\"intercept {}\".format(lr_rgr.intercept_))\n",
    "predict = lr_rgr.predict(X_test)\n",
    "LR_MSE = mean_squared_error(y_test, predict)\n",
    "print('Mean squared error for liner regression: {}'.format(LR_MSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.683398060975\n"
     ]
    }
   ],
   "source": [
    "# PolynomialFeatures\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import make_scorer, mean_squared_error\n",
    "\n",
    "mse = make_scorer(mean_squared_error, greater_is_better = False)\n",
    "poly = PolynomialFeatures(2)\n",
    "\n",
    "\n",
    "#Polynomial\n",
    "poly_rgr = linear_model.LinearRegression()\n",
    "poly_rgr.fit(poly.fit_transform(X_train),y_train)\n",
    "prediction = poly_rgr.predict(poly.fit_transform(X_test))\n",
    "Poly_MSE = mean_squared_error(y_test, prediction )\n",
    "print Poly_MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "733.236736432\n"
     ]
    }
   ],
   "source": [
    "poly3 = PolynomialFeatures(3)\n",
    "\n",
    "#Polynomial\n",
    "poly3_rgr = linear_model.LinearRegression()\n",
    "poly3_rgr.fit(poly3.fit_transform(X_train),y_train)\n",
    "Poly3_MSE = mean_squared_error(y_test, poly3_rgr.predict(poly3.fit_transform(X_test)))\n",
    "print Poly3_MSE\n",
    "                                                                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error for liner regression: 1.22325178741\n",
      "KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_neighbors=6, p=2, weights='uniform')\n",
      "{'n_neighbors': 6}\n",
      "<bound method GridSearchCV.score of GridSearchCV(cv=None, error_score='raise',\n",
      "       estimator=KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_neighbors=5, p=2, weights='uniform'),\n",
      "       fit_params={}, iid=True, loss_func=None, n_jobs=1,\n",
      "       param_grid=[{'n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]}],\n",
      "       pre_dispatch='2*n_jobs', refit=True, score_func=None, scoring=None,\n",
      "       verbose=0)>\n",
      "Mean squared error for liner regression: 1.20669829128\n"
     ]
    }
   ],
   "source": [
    "#KNN\n",
    "from sklearn import neighbors\n",
    "from sklearn import grid_search\n",
    "knn = neighbors.KNeighborsRegressor()\n",
    "knn.fit(X_train,y_train)\n",
    "knn_predict = knn.predict(X_test)\n",
    "LR_MSE = mean_squared_error(y_test, knn_predict)\n",
    "print('Mean squared error for liner regression: {}'.format(LR_MSE))\n",
    "param_grid = [{'n_neighbors':[1,2,3,4,5,6,7,8,9,10]}]\n",
    "knnrgr = grid_search.GridSearchCV(knn, param_grid)\n",
    "knnrgr.fit(X_train,y_train)\n",
    "print knnrgr.best_estimator_\n",
    "print knnrgr.best_params_\n",
    "print knnrgr.score\n",
    "knnrgrpredict = knnrgr.predict(X_test)\n",
    "LR_MSE2 = mean_squared_error(y_test, knnrgrpredict)\n",
    "print('Mean squared error for liner regression: {}'.format(LR_MSE2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.527053487027\n"
     ]
    }
   ],
   "source": [
    "#Ridge\n",
    "from sklearn import preprocessing\n",
    "from sklearn import grid_search\n",
    "param_grid = [{'alpha':np.linspace(1e-8, 1, 3000)}]\n",
    "X_train_scaled = preprocessing.scale(X_train)\n",
    "y_train_scaled = preprocessing.scale(y_train)\n",
    "X_test_scaled = preprocessing.scale(X_test)\n",
    "y_test_scaled = preprocessing.scale(y_test)\n",
    "#Ridge\n",
    "ridge_rgr = linear_model.Ridge(normalize = True)\n",
    "ridge_cv = grid_search.GridSearchCV(ridge_rgr, param_grid,cv=5)\n",
    "ridge_cv.fit(X_train_scaled, y_train_scaled)\n",
    "\n",
    "x_test_scaled_predict = ridge_cv.best_estimator_.predict(X_test_scaled)\n",
    "Ridge_MSE = mean_squared_error(y_test_scaled,x_test_scaled_predict )\n",
    "print Ridge_MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.492411948696\n"
     ]
    }
   ],
   "source": [
    "#lasso\n",
    "lasso_rgr = linear_model.Lasso(normalize = True)\n",
    "lasso_cv = grid_search.GridSearchCV(lasso_rgr, param_grid,cv=5)\n",
    "lasso_cv.fit(X_train_scaled, y_train_scaled)\n",
    "x_tested_lassopredict = lasso_cv.best_estimator_.predict(preprocessing.scale(X_test))\n",
    "Lasso_MSE = mean_squared_error(y_test_scaled,x_tested_lassopredict )\n",
    "print Lasso_MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "2.#Where there any features that were highly correlated?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lcavol</th>\n",
       "      <th>lweight</th>\n",
       "      <th>age</th>\n",
       "      <th>lbph</th>\n",
       "      <th>svi</th>\n",
       "      <th>gleason</th>\n",
       "      <th>pgg45</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lcavol</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.300232</td>\n",
       "      <td>0.286324</td>\n",
       "      <td>0.063168</td>\n",
       "      <td>0.592949</td>\n",
       "      <td>0.426414</td>\n",
       "      <td>0.483161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lweight</th>\n",
       "      <td>0.300232</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.316723</td>\n",
       "      <td>0.437042</td>\n",
       "      <td>0.181054</td>\n",
       "      <td>0.023558</td>\n",
       "      <td>0.074166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>0.286324</td>\n",
       "      <td>0.316723</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.287346</td>\n",
       "      <td>0.128902</td>\n",
       "      <td>0.365915</td>\n",
       "      <td>0.275806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lbph</th>\n",
       "      <td>0.063168</td>\n",
       "      <td>0.437042</td>\n",
       "      <td>0.287346</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.139147</td>\n",
       "      <td>0.032992</td>\n",
       "      <td>-0.030404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svi</th>\n",
       "      <td>0.592949</td>\n",
       "      <td>0.181054</td>\n",
       "      <td>0.128902</td>\n",
       "      <td>-0.139147</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.306875</td>\n",
       "      <td>0.481358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gleason</th>\n",
       "      <td>0.426414</td>\n",
       "      <td>0.023558</td>\n",
       "      <td>0.365915</td>\n",
       "      <td>0.032992</td>\n",
       "      <td>0.306875</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.757056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pgg45</th>\n",
       "      <td>0.483161</td>\n",
       "      <td>0.074166</td>\n",
       "      <td>0.275806</td>\n",
       "      <td>-0.030404</td>\n",
       "      <td>0.481358</td>\n",
       "      <td>0.757056</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           lcavol   lweight       age      lbph       svi   gleason     pgg45\n",
       "lcavol   1.000000  0.300232  0.286324  0.063168  0.592949  0.426414  0.483161\n",
       "lweight  0.300232  1.000000  0.316723  0.437042  0.181054  0.023558  0.074166\n",
       "age      0.286324  0.316723  1.000000  0.287346  0.128902  0.365915  0.275806\n",
       "lbph     0.063168  0.437042  0.287346  1.000000 -0.139147  0.032992 -0.030404\n",
       "svi      0.592949  0.181054  0.128902 -0.139147  1.000000  0.306875  0.481358\n",
       "gleason  0.426414  0.023558  0.365915  0.032992  0.306875  1.000000  0.757056\n",
       "pgg45    0.483161  0.074166  0.275806 -0.030404  0.481358  0.757056  1.000000"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gleason and pgg45 are highly correlated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.What features were the most important? Unimportant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.48908263  0.61223896 -0.01713236  0.15436282  0.55160293 -0.00848756\n",
      "   0.00550287]]\n",
      "['lcavol', 'lweight', 'age', 'lbph', 'svi', 'gleason', 'pgg45']\n"
     ]
    }
   ],
   "source": [
    "print lr_rgr.coef_\n",
    "feature_names= list(X_train.columns)\n",
    "print feature_names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lcvol has the highest positive influence\n",
    "age has the highest negative influence    they are the most inportanta\n",
    "the most unimportatnt feature is gleason\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.What model worked best? What was the MSE of each?\n",
    "the best is linear regression\n",
    "MSE are:   linear 0.46114\n",
    "           poly 0.68\n",
    "           knn   1.2\n",
    "           \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "difficulty cant figure out whats wrong with poly(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
